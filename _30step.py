IMPLEMENTATION_MODELS = [
    "gpt-oss:120b-cloud",
    "gemini-2.5-flash",
    "glm-4.6:cloud",
    "minimax-m2:cloud",
    "deepseek-v3.1:671b-cloud",
    "qwen3-coder:480b-cloud",
    "gemini-2.0-flash",
    "gemini-2.5-flash-lite-preview-09-2025",
    "gemini-2.5-pro",
    "gemini-2.0-flash-thinking-exp-01-21",
    "gemini-2.0-flash-001",
    "gemini-2.5-flash-lite-preview-06-17",
    "gemini-flash-lite-latest",
    "gemini-2.0-flash-thinking-exp-1219",
    "gemini-2.5-flash-lite-preview-09-2025",
    "gemini-2.0-flash",
    "gemini-2.5-pro",
    "gemini-2.5-flash-preview-05-20",
    "gemini-flash-latest",
    "gemini-2.0-flash-exp"
]

from typing import List, Dict, Tuple, Optional
import time
import asyncio
import json
import re
import os
from datetime import datetime
from cpp_pipe import (
    call_model_with_retry____________________________________________,
    extract_code_from_response,
    ExecutionMetrics,
    ModelConfig,
    PhaseResult,
    log_step,
    CppSandbox,
    _truncate_message
)

# API keys
api_key_google_list = [
    "AIzaSyBXC7krDh4mvI4VPKFUHpmkDrEcigOE00o",
    "AIzaSyAiSR_exmQehaC7Q0HPnuQUhr0S9jCtQFs",
    "AIzaSyAVhlHdikARNiTbyJLEBtExGBJPTCWucOg",
    "AIzaSyBt9wnZwb6gGm13gXIDLAs01JuF3PoSnBw",
]    

async def execute_and_fix(
    code: str,
    problem: str,
    tests: List[Dict],
    model_config: Dict,
    iteration: int
) -> Tuple[str, ExecutionMetrics]:
    """
    Execute code, capture errors, and fix with weak models
    
    Args:
        code: Current C++ code
        problem: Problem description
        tests: List of test cases
        model_config: Model configuration
        iteration: Current iteration number
        
    Returns:
        (fixed_code, metrics)
    """
    start_time = time.time()
    
    # Try to compile and execute
    with CppSandbox(timeout=5, memory_limit_mb=512) as sandbox:
        # Compile
        compile_success, compile_error = sandbox.compile(code)
        
        if not compile_success:
            # Code doesn't compile - ask model to fix
            system_prompt = "You are a C++ expert. Fix compilation errors."
            
            user_prompt = f"""This C++ code has compilation errors:

```cpp
{code}
```

Compilation Error:
{compile_error}

Fix the compilation errors and return the corrected code.
Return ONLY the fixed C++ code in ```cpp blocks."""
            
            try:
                response = await call_model_with_retry____________________________________________(
                    provider=model_config["provider"],
                    model=model_config["model"],
                    prompt=user_prompt,
                    system_prompt=system_prompt,
                    temperature=0.3
                )
                
                fixed_code = extract_code_from_response(response)
                
                metrics = ExecutionMetrics(
                    success=False,
                    output="",
                    error=compile_error,
                    time_ms=(time.time() - start_time) * 1000,
                    memory_mb=0.0,
                    timeout=False,
                    iteration=iteration,
                    model_used=model_config["name"],
                    test_passed=False,
                    timestamp=datetime.now().isoformat()
                )
                
                return fixed_code, metrics
                
            except Exception as e:
                metrics = ExecutionMetrics(
                    success=False,
                    output="",
                    error=f"Model error: {str(e)}",
                    time_ms=(time.time() - start_time) * 1000,
                    memory_mb=0.0,
                    timeout=False,
                    iteration=iteration,
                    model_used=model_config["name"],
                    test_passed=False,
                    timestamp=datetime.now().isoformat()
                )
                return code, metrics
        
        # Code compiles - run tests
        test_results = []
        all_passed = True
        
        for test in tests[:3]:  # Run first 3 tests
            test_input = test.get("input", "")
            expected_output = test.get("expected_output", "")
            
            exec_result = sandbox.execute(test_input)
            
            actual_output = exec_result["output"].strip()
            # Convert expected_output to string before calling strip()
            expected_str = str(expected_output).strip()
            passed = actual_output == expected_str
            
            test_results.append({
                "input": test_input,
                "expected": expected_output,
                "actual": actual_output,
                "passed": passed,
                "error": exec_result["error"]
            })
            
            if not passed:
                all_passed = False
        
        if all_passed:
            # All tests passed!
            metrics = ExecutionMetrics(
                success=True,
                output="All tests passed",
                error="",
                time_ms=(time.time() - start_time) * 1000,
                memory_mb=test_results[0].get("memory_mb", 0) if test_results else 0,
                timeout=False,
                iteration=iteration,
                model_used=model_config["name"],
                test_passed=True,
                timestamp=datetime.now().isoformat()
            )
            return code, metrics
        
        # Some tests failed - ask model to fix
        failed_tests = [t for t in test_results if not t["passed"]]
        
        system_prompt = "You are a C++ expert. Fix code to pass all test cases."
        
        user_prompt = f"""Problem:
{problem}

Current C++ Code:
```cpp
{code}
```

Failed Test Cases:
"""
        for i, test in enumerate(failed_tests[:2]):  # Show first 2 failures
            user_prompt += f"""
Test {i+1}:
Input: {test['input']}
Expected: {test['expected']}
Actual: {test['actual']}
Error: {test['error']}
"""
        
        user_prompt += """
Fix the code to pass all test cases correctly.
Return ONLY the fixed C++ code in ```cpp blocks."""
        
        try:
            response = await call_model_with_retry____________________________________________(
                provider=model_config["provider"],
                model=model_config["model"],
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3
            )
            
            fixed_code = extract_code_from_response(response)
            
            metrics = ExecutionMetrics(
                success=True,
                output=f"{len(failed_tests)} tests failed",
                error="",
                time_ms=(time.time() - start_time) * 1000,
                memory_mb=0.0,
                timeout=False,
                iteration=iteration,
                model_used=model_config["name"],
                test_passed=False,
                timestamp=datetime.now().isoformat()
            )
            
            return fixed_code, metrics
            
        except Exception as e:
            metrics = ExecutionMetrics(
                success=False,
                output="",
                error=f"Model error: {str(e)}",
                time_ms=(time.time() - start_time) * 1000,
                memory_mb=0.0,
                timeout=False,
                iteration=iteration,
                model_used=model_config["name"],
                test_passed=False,
                timestamp=datetime.now().isoformat()
            )
            return code, metrics


async def phase2_iterative_fixing(
    initial_code: str,
    problem: str,
    tests: List[Dict],
    max_iterations: int = 50
) -> PhaseResult:
    """
    Phase 2: Iteratively execute and fix code (Steps 21-70)
    
    Executes sequentially so each model can improve upon the previous model's solution
    """
    print(f"\n{'='*60}")
    print("PHASE 2: Iterative Fixing (Steps 21-70)")
    print(f"{'='*60}")
    
    phase_start = time.time()
    current_code = initial_code
    all_metrics = []
    
    for i in range(max_iterations):
        model_config = ModelConfig.WEAK_MODELS[i % len(ModelConfig.WEAK_MODELS)]
        step_num = 21 + i
        log_step(step_num, f"Iterative fixing with model {model_config['name']}")
        
        # Execute sequentially (await directly) so each model improves the previous result
        new_code, metrics = await execute_and_fix(
            current_code,
            problem,
            tests,
            model_config,
            step_num
        )
        
        all_metrics.append(metrics)
        
        if metrics.test_passed:
            log_step(step_num, "All monitored tests passed. Exiting iterative fixing early.")
            current_code = new_code
            break
        
        if not metrics.success and metrics.error:
            log_step(step_num, f"Execution/model error: {_truncate_message(metrics.error)}")
        
        if len(new_code) > 0:
            current_code = new_code
            log_step(step_num, f"Applied model suggestion ({len(current_code)} characters); continuing validation.")
        else:
            log_step(step_num, "No code changes produced; keeping previous version.")
        
        if metrics.output and not metrics.test_passed:
            log_step(step_num, f"Status: {_truncate_message(metrics.output)}")
        
        if (i + 1) % 10 == 0:
            print(f"  Progress: {i + 1}/{max_iterations} iterations completed")
    
    phase_duration = time.time() - phase_start
    
    print(f"\nâœ“ Completed {len(all_metrics)} sequential fix iterations")
    print(f"âœ“ Phase duration: {phase_duration:.2f}s")
    
    return PhaseResult(
        phase_name="Iterative Fixing",
        start_step=21,
        end_step=70,
        code=current_code,
        metrics=all_metrics,
        duration_seconds=phase_duration
    )


def _get_model_config(model_name: str) -> Dict:
    """
    Ø³Ø§Ø®Øª model config Ø§Ø² Ù†Ø§Ù… Ù…Ø¯Ù„
    
    Args:
        model_name: Ù†Ø§Ù… Ù…Ø¯Ù„
        
    Returns:
        Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ config Ù…Ø¯Ù„
    """
    provider = "google-genai" if model_name.startswith("gemini-") else "ollama"
    return {
        "provider": provider,
        "model": model_name.strip(),
        "name": model_name.strip()
    }


async def debug_and_fix_with_model(
    code: str,
    tests: List[Dict],
    model_config: Dict,
    step_num: int,
    api_key_google: str = ""
) -> Tuple[str, ExecutionMetrics]:
    """
    Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø¯ØŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø´Ú©Ù„Ø§Øª Ùˆ Ø±ÙØ¹ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø§ ÛŒÚ© Ù…Ø¯Ù„
    
    Args:
        code: Ú©Ø¯ C++ ÙØ¹Ù„ÛŒ
        tests: Ù„ÛŒØ³Øª ØªØ³Øª Ú©ÛŒØ³â€ŒÙ‡Ø§
        model_config: ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø¯Ù„
        step_num: Ø´Ù…Ø§Ø±Ù‡ Ú¯Ø§Ù…
        api_key_google: Ú©Ù„ÛŒØ¯ API Ú¯ÙˆÚ¯Ù„
        
    Returns:
        (Ú©Ø¯ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡, metrics)
    """
    start_time = time.time()
    
    # Ø§Ø¬Ø±Ø§ Ø¯Ø± sandbox
    with CppSandbox(timeout=5, memory_limit_mb=512) as sandbox:
        # Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„
        print(f"  ğŸ”¨ Compiling code...")
        compile_success, compile_error = sandbox.compile(code)
        
        if not compile_success:
            print(f"  âŒ Compilation failed!")
            print(f"  Error: {compile_error[:200]}")
            print(f"  ğŸ¤– Asking model to fix compilation errors...")
            # Ø®Ø·Ø§ÛŒ Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ - Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±ÙØ¹ Ø§Ø² Ù…Ø¯Ù„
            system_prompt = "You are a C++ expert. Fix ONLY compilation errors. Make MINIMAL changes to the code."
            
            user_prompt = f"""This C++ code has compilation errors. Fix ONLY the errors, do NOT make major changes:

```cpp
{code}
```

Compilation Error:
{compile_error}

Fix ONLY the compilation errors. Keep the code structure the same.
Return ONLY the fixed C++ code in ```cpp blocks."""
            
            try:
                response = await call_model_with_retry____________________________________________(
                    provider=model_config["provider"],
                    model=model_config["model"],
                    prompt=user_prompt,
                    system_prompt=system_prompt,
                    temperature=0.3,
                    api_key_google=api_key_google
                )
                
                fixed_code = extract_code_from_response(response)
                
                metrics = ExecutionMetrics(
                    success=False,
                    output="",
                    error=compile_error,
                    time_ms=(time.time() - start_time) * 1000,
                    memory_mb=0.0,
                    timeout=False,
                    iteration=step_num,
                    model_used=model_config["name"],
                    test_passed=False,
                    timestamp=datetime.now().isoformat()
                )
                
                return fixed_code if fixed_code else code, metrics
                
            except Exception as e:
                metrics = ExecutionMetrics(
                    success=False,
                    output="",
                    error=f"Model error: {str(e)}",
                    time_ms=(time.time() - start_time) * 1000,
                    memory_mb=0.0,
                    timeout=False,
                    iteration=step_num,
                    model_used=model_config["name"],
                    test_passed=False,
                    timestamp=datetime.now().isoformat()
                )
                return code, metrics
        
        # Ú©Ø¯ Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ø´Ø¯ - Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§
        print(f"  âœ… Compilation successful!")
        test_results = []
        all_passed = True
        
        print(f"  ğŸ§ª Running {len(tests)} test case(s)...")
        for i, test in enumerate(tests, 1):
            test_input = test.get("input", "")
            expected_output = test.get("expected_output", "")
            
            exec_result = sandbox.execute(test_input)
            
            actual_output = exec_result["output"].strip()
            expected_str = str(expected_output).strip()
            passed = actual_output == expected_str
            
            test_results.append({
                "input": test_input,
                "expected": expected_output,
                "actual": actual_output,
                "passed": passed,
                "error": exec_result["error"]
            })
            
            # Ù†Ù…Ø§ÛŒØ´ Ù†ØªÛŒØ¬Ù‡ Ù‡Ø± ØªØ³Øª
            status = "âœ…" if passed else "âŒ"
            print(f"    Test {i}: {status}", end="")
            if passed:
                print(f" Passed (Output: {actual_output[:50]})")
            else:
                print(f" Failed")
                print(f"      Expected: {expected_str[:100]}")
                print(f"      Actual:   {actual_output[:100]}")
                if exec_result["error"]:
                    print(f"      Error:    {exec_result['error'][:100]}")
            
            if not passed:
                all_passed = False
        
        if all_passed:
            # Ù‡Ù…Ù‡ ØªØ³Øªâ€ŒÙ‡Ø§ Ù¾Ø§Ø³ Ø´Ø¯Ù†Ø¯
            print(f"  âœ… All tests passed!")
            metrics = ExecutionMetrics(
                success=True,
                output="All tests passed",
                error="",
                time_ms=(time.time() - start_time) * 1000,
                memory_mb=test_results[0].get("memory_mb", 0) if test_results else 0,
                timeout=False,
                iteration=step_num,
                model_used=model_config["name"],
                test_passed=True,
                timestamp=datetime.now().isoformat()
            )
            return code, metrics
        
        # Ø¨Ø±Ø®ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ - Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø±ÙØ¹ Ø§Ø² Ù…Ø¯Ù„
        failed_tests = [t for t in test_results if not t["passed"]]
        
        print(f"  âŒ {len(failed_tests)} test(s) failed!")
        print(f"  ğŸ¤– Asking model to fix bugs...")
        
        system_prompt = "You are a C++ expert. Fix ONLY small bugs to make tests pass. Make MINIMAL changes."
        
        user_prompt = f"""This C++ code fails some test cases. Fix ONLY the bugs, do NOT rewrite the code:

```cpp
{code}
```

Failed Test Cases:
"""
        for i, test in enumerate(failed_tests[:3]):  # Ù†Ù…Ø§ÛŒØ´ 3 ØªØ³Øª Ù†Ø§Ù…ÙˆÙÙ‚ Ø§ÙˆÙ„
            user_prompt += f"""
Test {i+1}:
Input: {test['input']}
Expected: {test['expected']}
Actual: {test['actual']}
Error: {test['error']}
"""
        
        user_prompt += """
Fix ONLY the bugs to make all tests pass. Keep the code structure the same.
Return ONLY the fixed C++ code in ```cpp blocks."""
        
        try:
            response = await call_model_with_retry____________________________________________(
                provider=model_config["provider"],
                model=model_config["model"],
                prompt=user_prompt,
                system_prompt=system_prompt,
                temperature=0.3,
                api_key_google=api_key_google
            )
            
            fixed_code = extract_code_from_response(response)
            
            metrics = ExecutionMetrics(
                success=True,
                output=f"{len(failed_tests)} tests failed",
                error="",
                time_ms=(time.time() - start_time) * 1000,
                memory_mb=0.0,
                timeout=False,
                iteration=step_num,
                model_used=model_config["name"],
                test_passed=False,
                timestamp=datetime.now().isoformat()
            )
            
            return fixed_code if fixed_code else code, metrics
            
        except Exception as e:
            metrics = ExecutionMetrics(
                success=False,
                output="",
                error=f"Model error: {str(e)}",
                time_ms=(time.time() - start_time) * 1000,
                memory_mb=0.0,
                timeout=False,
                iteration=step_num,
                model_used=model_config["name"],
                test_passed=False,
                timestamp=datetime.now().isoformat()
            )
            return code, metrics


async def debug_with_all_models(
    code: str,
    tests: List[Dict],
    problem: str = "",
    api_key_google: Optional[str] = None
) -> Tuple[str, List[ExecutionMetrics]]:
    """
    Ø¯ÛŒØ¨Ø§Ú¯ Ùˆ Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª Ú©Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§
    Ù‡Ø± Ù…Ø¯Ù„ ÛŒÚ© Ú¯Ø§Ù… Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
    
    Args:
        code: Ú©Ø¯ C++ Ø§ÙˆÙ„ÛŒÙ‡
        tests: Ù„ÛŒØ³Øª ØªØ³Øª Ú©ÛŒØ³â€ŒÙ‡Ø§
        problem: ØªÙˆØ¶ÛŒØ­ Ù…Ø³Ø¦Ù„Ù‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        api_key_google: Ú©Ù„ÛŒØ¯ API Ú¯ÙˆÚ¯Ù„ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
        
    Returns:
        (Ú©Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡, Ù„ÛŒØ³Øª metrics)
    """
    print(f"\n{'='*60}")
    print(f"DEBUGGING WITH {len(IMPLEMENTATION_MODELS)} MODELS")
    print(f"{'='*60}")
    print(f"Starting with code length: {len(code)} characters")
    print(f"Test cases: {len(tests)}")
    print(f"{'='*60}\n")
    
    start_time = time.time()
    current_code = code
    all_metrics = []
    
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² API key Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡
    if api_key_google is None:
        api_key_google = api_key_google_list[0] if api_key_google_list else ""
    
    # Ù†Ù…Ø§ÛŒØ´ Ú©Ø¯ Ø§ÙˆÙ„ÛŒÙ‡
    print("\n" + "="*60)
    print("INITIAL CODE:")
    print("="*60)
    print(current_code)
    print("="*60 + "\n")
    
    # Ø§Ø¬Ø±Ø§ÛŒ ÛŒÚ© Ú¯Ø§Ù… Ø¨Ø§ Ù‡Ø± Ù…Ø¯Ù„
    for step, model_name in enumerate(IMPLEMENTATION_MODELS, start=1):
        print("\n" + "="*60)
        print(f"STEP {step}/{len(IMPLEMENTATION_MODELS)}")
        print("="*60)
        model_config = _get_model_config(model_name)
        log_step(step, f"Model: {model_config['name']}")
        log_step(step, f"Provider: {model_config['provider']}")
        
        # Ù†Ù…Ø§ÛŒØ´ Ú©Ø¯ ÙØ¹Ù„ÛŒ
        print(f"\nğŸ“ CURRENT CODE (Step {step}):")
        print("-" * 60)
        print(current_code)
        print("-" * 60)
        
        # Ø§Ø¬Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯ Ùˆ Ø±ÙØ¹ Ù…Ø´Ú©Ù„
        print(f"\nğŸ”„ Executing code and running tests...")
        new_code, metrics = await debug_and_fix_with_model(
            current_code,
            tests,
            model_config,
            step,
            api_key_google=api_key_google
        )
        
        all_metrics.append(metrics)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
        print(f"\nğŸ“Š RESULTS (Step {step}):")
        print("-" * 60)
        print(f"  Model: {model_config['name']}")
        print(f"  Success: {metrics.success}")
        print(f"  Test Passed: {metrics.test_passed}")
        print(f"  Time: {metrics.time_ms:.2f}ms")
        if metrics.error:
            print(f"  Error: {metrics.error[:200]}")
        if metrics.output:
            print(f"  Output: {metrics.output[:200]}")
        print("-" * 60)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù†ØªÛŒØ¬Ù‡
        if metrics.test_passed:
            print(f"\nâœ… SUCCESS! All tests passed at step {step}!")
            log_step(step, "âœ“ All tests passed! No further fixes needed.")
            current_code = new_code
            print(f"\nğŸ“ FINAL CODE (Step {step}):")
            print("-" * 60)
            print(current_code)
            print("-" * 60)
            break
        
        if not metrics.success and metrics.error:
            print(f"\nâš ï¸  ERROR at step {step}:")
            print(f"  {_truncate_message(metrics.error)}")
            log_step(step, f"âš  Error: {_truncate_message(metrics.error)}")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ØªØºÛŒÛŒØ±Ø§Øª Ú©Ø¯
        if len(new_code) > 0 and new_code != current_code:
            previous_code = current_code
            current_code = new_code
            print(f"\nâœï¸  CODE UPDATED at step {step}:")
            print(f"  Previous length: {len(previous_code)} chars")
            print(f"  New length: {len(new_code)} chars")
            print(f"\nğŸ“ NEW CODE (Step {step}):")
            print("-" * 60)
            print(new_code)
            print("-" * 60)
            log_step(step, f"âœ“ Code updated ({len(new_code)} characters)")
        else:
            print(f"\nâ—‹ No changes made at step {step}")
            log_step(step, "â—‹ No changes made")
        
        if metrics.output and not metrics.test_passed:
            print(f"  Status: {_truncate_message(metrics.output)}")
            log_step(step, f"Status: {_truncate_message(metrics.output)}")
        
        print("="*60)
    
    duration = time.time() - start_time
    
    print(f"\n{'='*60}")
    print("ğŸ‰ DEBUGGING COMPLETE")
    print(f"{'='*60}")
    print(f"ğŸ“Š SUMMARY:")
    print(f"  â€¢ Total steps executed: {len(all_metrics)}/{len(IMPLEMENTATION_MODELS)}")
    print(f"  â€¢ Total duration: {duration:.2f}s")
    print(f"  â€¢ Average time per step: {duration/len(all_metrics):.2f}s")
    print(f"  â€¢ Initial code length: {len(code)} characters")
    print(f"  â€¢ Final code length: {len(current_code)} characters")
    
    # Ø´Ù…Ø§Ø±Ø´ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§Ø³ Ø´Ø¯Ù‡
    passed_count = sum(1 for m in all_metrics if m.test_passed)
    if passed_count > 0:
        passed_steps = [i+1 for i, m in enumerate(all_metrics) if m.test_passed]
        print(f"  â€¢ âœ… Tests passed at step(s): {passed_steps}")
        print(f"  â€¢ ğŸ¯ First success at step: {passed_steps[0]}")
    else:
        print(f"  â€¢ âš ï¸  No steps achieved all tests passing")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§
    successful_models = [m.model_used for m in all_metrics if m.test_passed]
    if successful_models:
        print(f"  â€¢ ğŸ† Successful model(s): {successful_models[0]}")
    
    print(f"\nğŸ“ FINAL CODE:")
    print("-" * 60)
    print(current_code)
    print("-" * 60)
    print(f"{'='*60}\n")
    
    return current_code, all_metrics


def extract_codes_from_file(file_path: str) -> List[Dict[str, str]]:

    codes = []
    
    if not os.path.exists(file_path):
        print(f"âš ï¸  File not found: {file_path}")
        return codes
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‡Ù…Ù‡ RESULT Ù‡Ø§
    pattern = r'RESULT (\d+) \(from ([^)]+)\):\s*\n-+\s*\n(.*?)(?=\nRESULT \d+ \(from|$)'
    matches = re.finditer(pattern, content, re.DOTALL)
    
    for match in matches:
        result_num = match.group(1)
        model_name = match.group(2).strip()
        code = match.group(3).strip()
        
        # Ø­Ø°Ù Ú©Ø¯Ù‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù†Ø§Ù‚Øµ)

    
    return codes


def load_test_cases_from_json(json_path: str, index: int = 2) -> List[Dict]:

    if not os.path.exists(json_path):
        print(f"âš ï¸  File not found: {json_path}")
        return []
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if index >= len(data):
        print(f"âš ï¸  Index {index} out of range. Total problems: {len(data)}")
        return []
    
    problem = data[index]
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† samples
    samples = []
    if 'implementation_view' in problem:
        samples = problem['implementation_view'].get('samples', [])
    elif 'algorithm_view' in problem:
        samples = problem['algorithm_view'].get('samples', [])
    
    # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    test_cases = []
    for sample in samples:
        test_cases.append({
            'input': sample.get('input', ''),
            'expected_output': sample.get('output', '').strip()
        })
    
    return test_cases


# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ùˆ ØªØ³Øª
async def _30step(code: str, test: List[Dict],output_dir: str):

        # Ú©Ø¯ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
    sample_code = """#include <cstdio>
#    #include <cstdlib>
#    #include <algorithm>

/*---------------------------  Fast scanner  ---------------------------*/
#                   struct FastScanner {
    static const std::size_t SZ = 1 << 20;          // 1â€¯MiB buffer
    char buf[SZ];
    std::size_t idx = 0, len = 0;

    inline char readChar() {
        if (idx >= len) {
            len = fread(buf, 1, SZ, stdin);
            idx = 0;
            if (len == 0) return 0;               // EOF
        }
        return buf[idx++];
    }

    template <typename T>
    bool nextInt(T &out) {                         // reads signed integer
        char c = readChar();
        if (!c) return false;

        // skip nonâ€‘digit / nonâ€‘sign characters
        while (c != '-' && (c < '0' || c > '9')) {
            c = readChar();
            if (!c) return false;
        }

        bool neg = false;
        if (c == '-') {
            neg = true;
            c = readChar();
        }

        T val = 0;
        while (c >= '0' && c <= '9') {
            val = val * 10 + (c - '0');
            c = readChar();
        }
        out = neg ? -val : val;
        return true;
    }
};

/*---------------------------  Fast output  ---------------------------*/
static inline void writeLL(long long x) {
    char out[32];
    int p = 0;
    if (x == 0) {
        putchar_unlocked('0');
        putchar_unlocked('\n');
        return;
    }
    bool neg = x < 0;
    if (neg) x = -x;
    while (x) {
        out[p++] = char('0' + (x % 10));
        x /= 10;
    }
    if (neg) putchar_unlocked('-');
    while (p--) putchar_unlocked(out[p]);
    putchar_unlocked('\n');
}

/*---------------------------  Main  ---------------------------*/
int main() {
    FastScanner in;
    long long n_ll, k_ll;
    if (!in.nextInt(n_ll) || !in.nextInt(k_ll)) return 0;

    if (n_ll <= 0 || k_ll <= 0 || k_ll > n_ll) return 0;

    std::size_t n = static_cast<std::size_t>(n_ll);
    std::size_t k = static_cast<std::size_t>(k_ll);

    // allocate contiguous array
    long long *a = static_cast<long long*>(malloc(n * sizeof(long long)));
    if (!a) return 0;                     // allocation failure

    for (std::size_t i = 0; i < n; ++i) {
        if (!in.nextInt(a[i])) {          // malformed input
            free(a);
            return 0;
        }
    }

    // kâ€‘th largest â†’ element at position kâ€‘1 after nth_element with greater<>
    std::nth_element(a, a + (k - 1), a + n,
                     [](const long long &x, const long long &y) { return x > y; });

    writeLL(a[k - 1]);

    free(a);
    return 0;
}
"""
    
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØ³Øª Ú©ÛŒØ³â€ŒÙ‡Ø§
    #test_cases = [
    #    {
    #      "input": "6\n13 3 6 20 10 15\n0 0 0 1 1 2 2 1 0 0",
    #      "output": "35"
    #    }
    #  ]
    
 # Ø§Ø¬Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
    final_code, metrics = await debug_with_all_models(
                code=code,
                tests=test,
                problem="",
                api_key_google=api_key_google_list[1]
            )
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù†ØªÛŒØ¬Ù‡
    
    # Ù†ÙˆØ´ØªÙ† Ú©Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± ÙØ§ÛŒÙ„
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, "final_code.cpp")
    
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(final_code)
    
    print(f"\nâœ… Final code written to: {output_file}")
    
    return final_code



